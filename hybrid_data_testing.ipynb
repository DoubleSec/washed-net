{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for testing the hybrid tabular + sequential data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wn import net\n",
    "from wn.data import MatchHistoryDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorized data\n",
    "with open(\"data/tensor_list.pkl\", \"rb\") as f:\n",
    "    input_data, y = pickle.load(f)\n",
    "\n",
    "with open(\"data/history_tensor_list.pkl\", \"rb\") as f:\n",
    "    history_data, p1_id, p2_id = pickle.load(f)\n",
    "\n",
    "# Load the interfaces\n",
    "with open(\"data/match_interface.pkl\", \"rb\") as f:\n",
    "    match_interface = pickle.load(f)\n",
    "\n",
    "with open(\"data/history_interface.pkl\", \"rb\") as f:\n",
    "    history_interface = pickle.load(f)\n",
    "\n",
    "history_size = 40\n",
    "\n",
    "# Make a dataset\n",
    "ds = MatchHistoryDataset(input_data, y, history_data, p1_id, p2_id, history_size=history_size)\n",
    "\n",
    "# Split into training and validation\n",
    "idx = torch.randperm(len(input_data[\"p1_dob\"]))\n",
    "split_idx = idx.shape[0] // 4  # Just gets a 75/25 split, maximum laziness\n",
    "train_ds = Subset(ds, idx[split_idx:])\n",
    "validation_ds = Subset(ds, idx[:split_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network has 176705 weights.\n",
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set up the network for a test.\n",
    "\n",
    "col_encoding_size = 16\n",
    "dim_model = 64\n",
    "dim_ff = 64\n",
    "n_transformer_layers = 4\n",
    "n_transformer_heads = 4\n",
    "n_output_layers = 3\n",
    "\n",
    "# Special tabular input layer\n",
    "table_input_layer = net.TabularInputLayer(\n",
    "    interface=match_interface,\n",
    "    col_encoding_size=col_encoding_size,\n",
    "    embedding_size=dim_model - col_encoding_size,\n",
    "    append_cls=True,\n",
    ")\n",
    "\n",
    "# Input layer for sequential features, one for each player\n",
    "p1_sequence_input_layer = net.SequentialInputLayer(\n",
    "    interface=history_interface,\n",
    "    sequence_encoding_size=[history_size, col_encoding_size],\n",
    "    embedding_size=dim_model - col_encoding_size,\n",
    ")\n",
    "\n",
    "p2_sequence_input_layer = net.SequentialInputLayer(\n",
    "    interface=history_interface,\n",
    "    sequence_encoding_size=[history_size, col_encoding_size],\n",
    "    embedding_size=dim_model - col_encoding_size,\n",
    ")\n",
    "\n",
    "output_layers = net.OutputLayers(dim_model, n_output_layers, 1)\n",
    "\n",
    "# Transformer encoder\n",
    "tr = nn.TransformerEncoder(\n",
    "    encoder_layer=nn.TransformerEncoderLayer(\n",
    "        d_model=dim_model,\n",
    "        nhead=n_transformer_heads,\n",
    "        dim_feedforward=dim_ff,\n",
    "        batch_first=True,\n",
    "    ),\n",
    "    num_layers=n_transformer_layers,\n",
    ")\n",
    "\n",
    "whole_net = net.FusionNet(\n",
    "    table_input_layer=table_input_layer,\n",
    "    p1_sequence_input_layer=p1_sequence_input_layer,\n",
    "    p2_sequence_input_layer=p2_sequence_input_layer,\n",
    "    transformer=tr,\n",
    "    output_layer=output_layers,\n",
    ")\n",
    "\n",
    "n_weights = sum([p.numel() for p in whole_net.parameters() if p.requires_grad])\n",
    "print(f\"Network has {n_weights} weights.\")\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "whole_net.to(device)\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 244 batches of size 1024\n",
      "Validation: 82 batches\n",
      "Starting epoch  1 ------\n",
      "Epoch 1, Batch   50: 0.672 |  Accuracy: 0.58 |    2936 obs/sec |  28.95 average history | 17.44 s\n",
      "Epoch 1, Batch  100: 0.652 |  Accuracy: 0.61 |    3760 obs/sec |  28.96 average history | 31.06 s\n",
      "Epoch 1, Batch  150: 0.648 |  Accuracy: 0.61 |    3777 obs/sec |  29.12 average history | 44.62 s\n",
      "Epoch 1, Batch  200: 0.644 |  Accuracy: 0.62 |    3739 obs/sec |  29.00 average history | 58.31 s\n",
      "Epoch 1 validation loss: 0.644 |  Accuracy: 0.62 |    3680 obs/sec |  92.88 s\n",
      "Starting epoch  2 ------\n",
      "Epoch 2, Batch   50: 0.645 |  Accuracy: 0.62 |    3116 obs/sec |  29.04 average history | 109.31 s\n",
      "Epoch 2, Batch  100: 0.642 |  Accuracy: 0.63 |    3703 obs/sec |  29.03 average history | 123.13 s\n",
      "Epoch 2, Batch  150: 0.642 |  Accuracy: 0.63 |    3842 obs/sec |  28.95 average history | 136.46 s\n",
      "Epoch 2, Batch  200: 0.641 |  Accuracy: 0.63 |    3681 obs/sec |  29.06 average history | 150.37 s\n",
      "Epoch 2 validation loss: 0.641 |  Accuracy: 0.63 |    3711 obs/sec |  184.64 s\n",
      "Starting epoch  3 ------\n",
      "Epoch 3, Batch   50: 0.640 |  Accuracy: 0.63 |    3330 obs/sec |  29.13 average history | 200.01 s\n",
      "Epoch 3, Batch  100: 0.639 |  Accuracy: 0.63 |    3764 obs/sec |  28.96 average history | 213.61 s\n",
      "Epoch 3, Batch  150: 0.638 |  Accuracy: 0.64 |    3832 obs/sec |  29.02 average history | 226.97 s\n",
      "Epoch 3, Batch  200: 0.640 |  Accuracy: 0.63 |    3703 obs/sec |  29.02 average history | 240.80 s\n",
      "Epoch 3 validation loss: 0.637 |  Accuracy: 0.64 |    3071 obs/sec |  280.05 s\n",
      "Starting epoch  4 ------\n",
      "Epoch 4, Batch   50: 0.638 |  Accuracy: 0.64 |    3096 obs/sec |  29.05 average history | 296.59 s\n",
      "Epoch 4, Batch  100: 0.635 |  Accuracy: 0.64 |    3677 obs/sec |  29.02 average history | 310.51 s\n",
      "Epoch 4, Batch  150: 0.633 |  Accuracy: 0.65 |    3778 obs/sec |  28.94 average history | 324.06 s\n",
      "Epoch 4, Batch  200: 0.634 |  Accuracy: 0.64 |    3741 obs/sec |  29.11 average history | 337.75 s\n",
      "Epoch 4 validation loss: 0.634 |  Accuracy: 0.64 |    3625 obs/sec |  372.64 s\n",
      "Starting epoch  5 ------\n",
      "Epoch 5, Batch   50: 0.632 |  Accuracy: 0.65 |    3151 obs/sec |  28.95 average history | 388.89 s\n",
      "Epoch 5, Batch  100: 0.633 |  Accuracy: 0.64 |    3704 obs/sec |  28.99 average history | 402.72 s\n",
      "Epoch 5, Batch  150: 0.631 |  Accuracy: 0.65 |    3825 obs/sec |  29.11 average history | 416.10 s\n",
      "Epoch 5, Batch  200: 0.633 |  Accuracy: 0.64 |    3715 obs/sec |  29.11 average history | 429.89 s\n",
      "Epoch 5 validation loss: 0.630 |  Accuracy: 0.65 |    3727 obs/sec |  464.11 s\n",
      "Starting epoch  6 ------\n",
      "Epoch 6, Batch   50: 0.627 |  Accuracy: 0.65 |    3326 obs/sec |  29.11 average history | 479.50 s\n",
      "Epoch 6, Batch  100: 0.626 |  Accuracy: 0.65 |    3768 obs/sec |  28.94 average history | 493.09 s\n",
      "Epoch 6, Batch  150: 0.627 |  Accuracy: 0.65 |    3831 obs/sec |  29.04 average history | 506.46 s\n",
      "Epoch 6, Batch  200: 0.627 |  Accuracy: 0.65 |    3762 obs/sec |  29.03 average history | 520.07 s\n",
      "Epoch 6 validation loss: 0.632 |  Accuracy: 0.64 |    3698 obs/sec |  553.90 s\n",
      "Starting epoch  7 ------\n",
      "Epoch 7, Batch   50: 0.626 |  Accuracy: 0.65 |    3143 obs/sec |  29.02 average history | 570.19 s\n",
      "Epoch 7, Batch  100: 0.625 |  Accuracy: 0.65 |    3897 obs/sec |  29.03 average history | 583.33 s\n",
      "Epoch 7, Batch  150: 0.624 |  Accuracy: 0.65 |    3741 obs/sec |  29.09 average history | 597.02 s\n",
      "Epoch 7, Batch  200: 0.623 |  Accuracy: 0.65 |    3703 obs/sec |  28.98 average history | 610.85 s\n",
      "Epoch 7 validation loss: 0.626 |  Accuracy: 0.65 |    3712 obs/sec |  645.22 s\n",
      "Starting epoch  8 ------\n",
      "Epoch 8, Batch   50: 0.625 |  Accuracy: 0.65 |    3138 obs/sec |  29.03 average history | 661.53 s\n",
      "Epoch 8, Batch  100: 0.624 |  Accuracy: 0.65 |    3691 obs/sec |  29.01 average history | 675.41 s\n",
      "Epoch 8, Batch  150: 0.624 |  Accuracy: 0.65 |    3873 obs/sec |  29.01 average history | 688.62 s\n",
      "Epoch 8, Batch  200: 0.623 |  Accuracy: 0.65 |    3793 obs/sec |  29.05 average history | 702.12 s\n",
      "Epoch 8 validation loss: 0.625 |  Accuracy: 0.65 |    3640 obs/sec |  737.01 s\n",
      "Starting epoch  9 ------\n",
      "Epoch 9, Batch   50: 0.623 |  Accuracy: 0.65 |    3139 obs/sec |  29.02 average history | 753.32 s\n",
      "Epoch 9, Batch  100: 0.623 |  Accuracy: 0.65 |    3722 obs/sec |  29.04 average history | 767.07 s\n",
      "Epoch 9, Batch  150: 0.624 |  Accuracy: 0.65 |    3800 obs/sec |  29.01 average history | 780.55 s\n",
      "Epoch 9, Batch  200: 0.623 |  Accuracy: 0.65 |    3700 obs/sec |  29.05 average history | 794.39 s\n",
      "Epoch 9 validation loss: 0.623 |  Accuracy: 0.65 |    3664 obs/sec |  828.13 s\n",
      "Starting epoch 10 ------\n",
      "Epoch 10, Batch   50: 0.620 |  Accuracy: 0.65 |    3141 obs/sec |  28.96 average history | 844.43 s\n",
      "Epoch 10, Batch  100: 0.620 |  Accuracy: 0.65 |    3828 obs/sec |  29.03 average history | 857.81 s\n",
      "Epoch 10, Batch  150: 0.621 |  Accuracy: 0.65 |    3942 obs/sec |  28.92 average history | 870.80 s\n",
      "Epoch 10, Batch  200: 0.624 |  Accuracy: 0.65 |    3714 obs/sec |  29.15 average history | 884.58 s\n",
      "Epoch 10 validation loss: 0.621 |  Accuracy: 0.65 |    3670 obs/sec |  919.12 s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# Training parameters. Initially lifted from the simpler network.\n",
    "\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Create a dataloader, optimizer, and criterion\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "validation_dl = DataLoader(\n",
    "    validation_ds, batch_size=batch_size, shuffle=True, num_workers=3\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(train_dl)} batches of size {batch_size}\")\n",
    "print(f\"Validation: {len(validation_dl)} batches\")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, whole_net.parameters()),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "# Per epoch\n",
    "big_tick = perf_counter()\n",
    "\n",
    "# For this amount of data, 5 epochs gives a decent indication of what kind\n",
    "# of performance to expect, it seems.\n",
    "for epoch in range(10):\n",
    "\n",
    "    print(f\"Starting epoch {epoch+1 :2} ------\")\n",
    "\n",
    "    # Training\n",
    "\n",
    "    whole_net.train()\n",
    "\n",
    "    tick = perf_counter()\n",
    "    running_loss = 0.0\n",
    "    running_n = 0\n",
    "    running_correct = 0\n",
    "    running_available_history = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dl):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get a batch\n",
    "        mx, sx1, mask1, sx2, mask2, y = batch\n",
    "\n",
    "        mx = net.to_(mx, device)\n",
    "        sx1 = net.to_(sx1, device)\n",
    "        mask1 = mask1.to(device)\n",
    "        sx2 = net.to_(sx2, device)\n",
    "        mask2 = mask2.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_hat = whole_net(mx, sx1, mask1, sx2, mask2)\n",
    "        labels = y_hat > 0\n",
    "        correct = (labels == y).sum()\n",
    "\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_available_history += mask1.sum().item() + mask2.sum().item()\n",
    "\n",
    "        running_correct += correct.item()\n",
    "        running_loss += loss.item()\n",
    "        running_n += y_hat.shape[0]\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}, Batch {i+1 :4}: {running_loss / running_n :.3f} | \",\n",
    "                f\"Accuracy: {running_correct / running_n :.2f} | \",\n",
    "                f\"{running_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "                f\"{running_available_history / (2 * running_n) :.2f} average history | \"\n",
    "                f\"{perf_counter() - big_tick :.2f} s\",\n",
    "            )\n",
    "            running_available_history = 0\n",
    "            running_loss = 0.0\n",
    "            running_n = 0\n",
    "            running_correct = 0\n",
    "            tick = perf_counter()\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    whole_net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tick = perf_counter()\n",
    "        valid_loss = 0.0\n",
    "        valid_n = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        for i, batch in enumerate(validation_dl):\n",
    "\n",
    "            # Get a batch\n",
    "            mx, sx1, mask1, sx2, mask2, y = batch\n",
    "\n",
    "            mx = net.to_(mx, device)\n",
    "            sx1 = net.to_(sx1, device)\n",
    "            mask1 = mask1.to(device)\n",
    "            sx2 = net.to_(sx2, device)\n",
    "            mask2 = mask2.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = whole_net(mx, sx1, mask1, sx2, mask2)\n",
    "            labels = y_hat > 0\n",
    "            correct = (labels == y).sum()\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            valid_correct += correct.item()\n",
    "            valid_loss += loss.item()\n",
    "            valid_n += y_hat.shape[0]\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} validation loss: {running_loss / running_n :.3f} | \",\n",
    "            f\"Accuracy: {valid_correct / valid_n :.2f} | \",\n",
    "            f\"{valid_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "            f\"{perf_counter() - big_tick :.2f} s\",\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aaadde310f7da2af25cef5f777dce8f13f327f179bdb50a63ba144a6985ebf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
