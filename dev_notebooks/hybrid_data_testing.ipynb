{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for testing the hybrid tabular + sequential data representation.\n",
    "\n",
    "**Please note: this notebook is superseded by `hybrid_training.py`, because environment variables are too irritating in notebooks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wn import net\n",
    "from wn.data import MatchHistoryDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may not have mflow setup, in which case you can skip all of this\n",
    "import mlflow\n",
    "\n",
    "experiment_name = \"washed_net\"\n",
    "\n",
    "# Auth handled with env variables.\n",
    "mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"Using MLflow experiment: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the training parameters up here.\n",
    "\n",
    "# I don't want to type training_parameters a bunch of times...\n",
    "tp = {\n",
    "    # Data parameters\n",
    "    \"history_length\": 0,  # How many matches of history are provided\n",
    "    \"validation_set_size\": 0.25,  # Ratio for train/test split.\n",
    "    # Network architecture\n",
    "    \"col_encoding_size\": 16,  # Size of learned column/position encodings\n",
    "    \"dim_model\": 64,  # Size of many layers in net\n",
    "    \"dim_ff\": 64,  # Size of linear layers in transformer\n",
    "    \"n_transformer_layers\": 4,  # Transformer depth\n",
    "    \"n_transformer_heads\": 4,  # Attention heads, must divide dim_model\n",
    "    \"n_output_layers\": 3,  # Depth of linear output layers\n",
    "    # Training parameters\n",
    "    \"batch_size\": 1024,  # Batch size\n",
    "    \"learning_rate\": 0.0001,  # Learning rate, currently just fixed\n",
    "    \"n_epochs\": 15,  # Training epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorized data\n",
    "with open(\"../data/tensor_list.pkl\", \"rb\") as f:\n",
    "    input_data, y = pickle.load(f)\n",
    "\n",
    "with open(\"../data/history_tensor_list.pkl\", \"rb\") as f:\n",
    "    history_data, p1_id, p2_id = pickle.load(f)\n",
    "\n",
    "# Load the interfaces\n",
    "with open(\"../data/match_interface.pkl\", \"rb\") as f:\n",
    "    match_interface = pickle.load(f)\n",
    "\n",
    "with open(\"../data/history_interface.pkl\", \"rb\") as f:\n",
    "    history_interface = pickle.load(f)\n",
    "\n",
    "# Make a dataset\n",
    "ds = MatchHistoryDataset(\n",
    "    input_data, y, history_data, p1_id, p2_id, history_size=tp[\"history_length\"]\n",
    ")\n",
    "\n",
    "# Split into training and validation\n",
    "idx = torch.randperm(len(input_data[\"p1_dob\"]))\n",
    "split_idx = int(idx.shape[0] * tp[\"validation_set_size\"])\n",
    "train_ds = Subset(ds, idx[split_idx:])\n",
    "validation_ds = Subset(ds, idx[:split_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the network for a test.\n",
    "\n",
    "# Special tabular input layer\n",
    "table_input_layer = net.TabularInputLayer(\n",
    "    interface=match_interface,\n",
    "    col_encoding_size=tp[\"col_encoding_size\"],\n",
    "    embedding_size=tp[\"dim_model\"] - tp[\"col_encoding_size\"],\n",
    "    append_cls=True,\n",
    ")\n",
    "\n",
    "# Input layer for sequential features, one for each player\n",
    "p1_sequence_input_layer = net.SequentialInputLayer(\n",
    "    interface=history_interface,\n",
    "    sequence_encoding_size=[tp[\"history_length\"], tp[\"col_encoding_size\"]],\n",
    "    embedding_size=tp[\"dim_model\"] - tp[\"col_encoding_size\"],\n",
    ")\n",
    "\n",
    "p2_sequence_input_layer = net.SequentialInputLayer(\n",
    "    interface=history_interface,\n",
    "    sequence_encoding_size=[tp[\"history_length\"], tp[\"col_encoding_size\"]],\n",
    "    embedding_size=tp[\"dim_model\"] - tp[\"col_encoding_size\"],\n",
    ")\n",
    "\n",
    "output_layers = net.OutputLayers(tp[\"dim_model\"], tp[\"n_output_layers\"], 1)\n",
    "\n",
    "# Transformer encoder\n",
    "tr = nn.TransformerEncoder(\n",
    "    encoder_layer=nn.TransformerEncoderLayer(\n",
    "        d_model=tp[\"dim_model\"],\n",
    "        nhead=tp[\"n_transformer_heads\"],\n",
    "        dim_feedforward=tp[\"dim_ff\"],\n",
    "        batch_first=True,\n",
    "    ),\n",
    "    num_layers=tp[\"n_transformer_layers\"],\n",
    ")\n",
    "\n",
    "whole_net = net.FusionNet(\n",
    "    table_input_layer=table_input_layer,\n",
    "    p1_sequence_input_layer=p1_sequence_input_layer,\n",
    "    p2_sequence_input_layer=p2_sequence_input_layer,\n",
    "    transformer=tr,\n",
    "    output_layer=output_layers,\n",
    ")\n",
    "\n",
    "n_weights = sum([p.numel() for p in whole_net.parameters() if p.requires_grad])\n",
    "print(f\"Network has {n_weights} weights.\")\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "whole_net.to(device)\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Write all the parameters to mlflow:\n",
    "    mlflow.log_params(tp)\n",
    "    mlflow.log_param(\"n_weights\", n_weights)\n",
    "\n",
    "    # Create a dataloader, optimizer, and criterion\n",
    "\n",
    "    train_dl = DataLoader(\n",
    "        train_ds, batch_size=tp[\"batch_size\"], shuffle=True, num_workers=3\n",
    "    )\n",
    "    validation_dl = DataLoader(\n",
    "        validation_ds, batch_size=tp[\"batch_size\"], shuffle=True, num_workers=3\n",
    "    )\n",
    "\n",
    "    print(f\"Training: {len(train_dl)} batches of size {tp['batch_size']}\")\n",
    "    print(f\"Validation: {len(validation_dl)} batches\")\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        filter(lambda p: p.requires_grad, whole_net.parameters()),\n",
    "        lr=tp[\"learning_rate\"],\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "    # For tracking\n",
    "    big_tick = perf_counter()\n",
    "    n_training_obs = 0\n",
    "\n",
    "    for epoch in range(tp[\"n_epochs\"]):\n",
    "\n",
    "        print(f\"Starting epoch {epoch+1 :2} ------\")\n",
    "\n",
    "        # Training\n",
    "\n",
    "        whole_net.train()\n",
    "\n",
    "        tick = perf_counter()\n",
    "        running_loss = 0.0\n",
    "        running_n = 0\n",
    "        running_correct = 0\n",
    "        running_available_history = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dl):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get a batch\n",
    "            mx, sx1, mask1, sx2, mask2, y = batch\n",
    "\n",
    "            mx = net.to_(mx, device)\n",
    "            sx1 = net.to_(sx1, device)\n",
    "            mask1 = mask1.to(device)\n",
    "            sx2 = net.to_(sx2, device)\n",
    "            mask2 = mask2.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = whole_net(mx, sx1, mask1, sx2, mask2)\n",
    "            labels = y_hat > 0\n",
    "            correct = (labels == y).sum()\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_available_history += mask1.sum().item() + mask2.sum().item()\n",
    "\n",
    "            n_training_obs += y_hat.shape[0]\n",
    "            running_correct += correct.item()\n",
    "            running_loss += loss.item()\n",
    "            running_n += y_hat.shape[0]\n",
    "\n",
    "            # mlflow logging\n",
    "            if i % 10 == 9:\n",
    "                mlflow.log_metrics(\n",
    "                    {\n",
    "                        \"train_loss\": running_loss / running_n,\n",
    "                        \"train_accuracy\": running_correct / running_n,\n",
    "                    },\n",
    "                    step=n_training_obs,\n",
    "                )\n",
    "\n",
    "            # Print\n",
    "            if i % 100 == 99:\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}, Batch {i+1 :4}: {running_loss / running_n :.3f} | \",\n",
    "                    f\"Accuracy: {running_correct / running_n :.3f} | \",\n",
    "                    f\"{running_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "                    f\"{running_available_history / (2 * running_n) :.2f} average history | \"\n",
    "                    f\"{perf_counter() - big_tick :.2f} s\",\n",
    "                )\n",
    "                running_available_history = 0\n",
    "                running_loss = 0.0\n",
    "                running_n = 0\n",
    "                running_correct = 0\n",
    "                tick = perf_counter()\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        whole_net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            tick = perf_counter()\n",
    "            valid_loss = 0.0\n",
    "            valid_n = 0\n",
    "            valid_correct = 0\n",
    "\n",
    "            for i, batch in enumerate(validation_dl):\n",
    "\n",
    "                # Get a batch\n",
    "                mx, sx1, mask1, sx2, mask2, y = batch\n",
    "\n",
    "                mx = net.to_(mx, device)\n",
    "                sx1 = net.to_(sx1, device)\n",
    "                mask1 = mask1.to(device)\n",
    "                sx2 = net.to_(sx2, device)\n",
    "                mask2 = mask2.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                y_hat = whole_net(mx, sx1, mask1, sx2, mask2)\n",
    "                labels = y_hat > 0\n",
    "                correct = (labels == y).sum()\n",
    "\n",
    "                loss = criterion(y_hat, y)\n",
    "\n",
    "                valid_correct += correct.item()\n",
    "                valid_loss += loss.item()\n",
    "                valid_n += y_hat.shape[0]\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} validation loss: {valid_loss / valid_n :.3f} | \",\n",
    "                f\"Accuracy: {valid_correct / valid_n :.3f} | \",\n",
    "                f\"{valid_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "                f\"{perf_counter() - big_tick :.2f} s\",\n",
    "            )\n",
    "\n",
    "            # mlflow logging\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"validation_loss\": valid_correct / valid_n,\n",
    "                    \"validation_accuracy\": valid_correct / valid_n,\n",
    "                },\n",
    "                step=n_training_obs,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aaadde310f7da2af25cef5f777dce8f13f327f179bdb50a63ba144a6985ebf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
