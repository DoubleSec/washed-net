{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wn import net\n",
    "from wn.data import MatchDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorized data\n",
    "with open(\"data/tensor_list.pkl\", \"rb\") as f:\n",
    "    input_data, y  = pickle.load(f)\n",
    "\n",
    "ds = MatchDataset(input_data, y)\n",
    "\n",
    "# Split into training and validation\n",
    "idx = torch.randperm(len(input_data[\"p1_dob\"]))\n",
    "split_idx = idx.shape[0] // 4\n",
    "train_ds = Subset(ds, idx[split_idx:])\n",
    "validation_ds = Subset(ds, idx[:split_idx])\n",
    "\n",
    "# Load the interface\n",
    "with open(\"data/match_interface.pkl\", \"rb\") as f:\n",
    "    match_interface = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network has 123841 weights.\n",
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set up the network for a test.\n",
    "\n",
    "col_encoding_size = 16\n",
    "dim_model = 64\n",
    "dim_ff = 64\n",
    "n_transformer_layers = 4\n",
    "n_transformer_heads = 4\n",
    "n_output_layers = 3\n",
    "\n",
    "# Special tabular input layer\n",
    "input_layer = net.TabularInputLayer(\n",
    "    interface=match_interface,\n",
    "    col_encoding_size=col_encoding_size,\n",
    "    embedding_size=dim_model-col_encoding_size,\n",
    "    append_cls=True,\n",
    ")\n",
    "\n",
    "# Transformer encoder\n",
    "tr = nn.TransformerEncoder(\n",
    "    encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model=dim_model,\n",
    "        nhead=n_transformer_heads,\n",
    "        dim_feedforward=dim_ff,\n",
    "        batch_first=True,\n",
    "    ),\n",
    "    num_layers=n_transformer_layers,\n",
    ")\n",
    "\n",
    "# Ugly convenience layer\n",
    "cls_selector = net.SelectCLSEncoding()\n",
    "\n",
    "# Output layer\n",
    "output_layers = net.OutputLayers(dim_model, n_output_layers, 1)\n",
    "\n",
    "whole_net = nn.Sequential(\n",
    "    input_layer,\n",
    "    tr,\n",
    "    cls_selector,\n",
    "    output_layers,\n",
    ")\n",
    "\n",
    "n_weights = sum([p.numel() for p in whole_net.parameters() if p.requires_grad])\n",
    "print(f\"Network has {n_weights} weights.\")\n",
    "\n",
    "# Setup device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "whole_net.to(device)\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 244 batches of size 1024\n",
      "Validation: 82 batches\n",
      "Starting epoch  1 ------\n",
      "Epoch 1, Batch   50: 0.700 |  Accuracy: 0.51 |    9740 obs/sec |  5.26 s\n",
      "Epoch 1, Batch  100: 0.698 |  Accuracy: 0.50 |   33402 obs/sec |  6.79 s\n",
      "Epoch 1, Batch  150: 0.694 |  Accuracy: 0.51 |   33532 obs/sec |  8.32 s\n",
      "Epoch 1, Batch  200: 0.692 |  Accuracy: 0.52 |   33687 obs/sec |  9.84 s\n",
      "Epoch 1 validation loss: 0.687 |  Accuracy: 0.57 |   19270 obs/sec |  15.77 s\n",
      "Starting epoch  2 ------\n",
      "Epoch 2, Batch   50: 0.683 |  Accuracy: 0.56 |   13963 obs/sec |  19.44 s\n",
      "Epoch 2, Batch  100: 0.683 |  Accuracy: 0.55 |   33690 obs/sec |  20.96 s\n",
      "Epoch 2, Batch  150: 0.680 |  Accuracy: 0.56 |   32220 obs/sec |  22.55 s\n",
      "Epoch 2, Batch  200: 0.679 |  Accuracy: 0.57 |   32947 obs/sec |  24.10 s\n",
      "Epoch 2 validation loss: 0.676 |  Accuracy: 0.59 |   18746 obs/sec |  30.14 s\n",
      "Starting epoch  3 ------\n",
      "Epoch 3, Batch   50: 0.675 |  Accuracy: 0.57 |   13837 obs/sec |  33.84 s\n",
      "Epoch 3, Batch  100: 0.671 |  Accuracy: 0.58 |   33197 obs/sec |  35.38 s\n",
      "Epoch 3, Batch  150: 0.668 |  Accuracy: 0.58 |   33679 obs/sec |  36.90 s\n",
      "Epoch 3, Batch  200: 0.667 |  Accuracy: 0.58 |   33200 obs/sec |  38.44 s\n",
      "Epoch 3 validation loss: 0.666 |  Accuracy: 0.60 |   19490 obs/sec |  44.34 s\n",
      "Starting epoch  4 ------\n",
      "Epoch 4, Batch   50: 0.665 |  Accuracy: 0.59 |   13632 obs/sec |  48.10 s\n",
      "Epoch 4, Batch  100: 0.669 |  Accuracy: 0.58 |   33300 obs/sec |  49.64 s\n",
      "Epoch 4, Batch  150: 0.666 |  Accuracy: 0.59 |   33374 obs/sec |  51.17 s\n",
      "Epoch 4, Batch  200: 0.667 |  Accuracy: 0.59 |   33459 obs/sec |  52.70 s\n",
      "Epoch 4 validation loss: 0.666 |  Accuracy: 0.61 |   19045 obs/sec |  58.68 s\n",
      "Starting epoch  5 ------\n",
      "Epoch 5, Batch   50: 0.665 |  Accuracy: 0.59 |   13736 obs/sec |  62.41 s\n",
      "Epoch 5, Batch  100: 0.666 |  Accuracy: 0.59 |   33481 obs/sec |  63.94 s\n",
      "Epoch 5, Batch  150: 0.663 |  Accuracy: 0.59 |   33313 obs/sec |  65.48 s\n",
      "Epoch 5, Batch  200: 0.664 |  Accuracy: 0.59 |   33525 obs/sec |  67.01 s\n",
      "Epoch 5 validation loss: 0.663 |  Accuracy: 0.62 |   19118 obs/sec |  72.97 s\n"
     ]
    }
   ],
   "source": [
    "# Example training step\n",
    "# Create a dataloader, optimizer, and criterion\n",
    "\n",
    "# Large batches and low learning rates seem basically required for this network\n",
    "# to reliably learn anything. Small batches just don't learn at all, large\n",
    "# batches with higher learning rates have a tendency to spontaneously forget\n",
    "# everything during training and then can't recover.\n",
    "batch_size = 1024\n",
    "learning_rate = 0.0001\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "validation_dl = DataLoader(validation_ds, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "print(f\"Training: {len(train_dl)} batches of size {batch_size}\")\n",
    "print(f\"Validation: {len(validation_dl)} batches\")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, whole_net.parameters()),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "# Per epoch\n",
    "big_tick = perf_counter()\n",
    "\n",
    "# For this amount of data, 5 epochs gives a decent indication of what kind\n",
    "# of performance to expect, it seems.\n",
    "for epoch in range(5):\n",
    "\n",
    "    print(f\"Starting epoch {epoch+1 :2} ------\")\n",
    "\n",
    "    # Training\n",
    "\n",
    "    whole_net.train()\n",
    "\n",
    "    tick = perf_counter()\n",
    "    running_loss = 0.0\n",
    "    running_n = 0\n",
    "    running_correct = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dl):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get a batch\n",
    "        x, y = batch\n",
    "\n",
    "        x = net.to_(x, device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_hat = whole_net(x)\n",
    "        labels = y_hat > 0\n",
    "        correct = (labels == y).sum()\n",
    "\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_correct += correct.item()\n",
    "        running_loss += loss.item()\n",
    "        running_n += y_hat.shape[0]\n",
    "        \n",
    "        if i % 50 == 49:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}, Batch {i+1 :4}: {running_loss / running_n :.3f} | \",\n",
    "                f\"Accuracy: {running_correct / running_n :.2f} | \",\n",
    "                f\"{running_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "                f\"{perf_counter() - big_tick :.2f} s\"\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            running_n = 0\n",
    "            running_correct = 0\n",
    "            tick = perf_counter()\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    whole_net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        tick = perf_counter()\n",
    "        valid_loss = 0.0\n",
    "        valid_n = 0\n",
    "        valid_correct = 0\n",
    "\n",
    "        for i, batch in enumerate(validation_dl):\n",
    "\n",
    "            # Get a batch\n",
    "            x, y = batch\n",
    "\n",
    "            x = net.to_(x, device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = whole_net(x)\n",
    "            labels = y_hat > 0\n",
    "            correct = (labels == y).sum()\n",
    "\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            valid_correct += correct.item()\n",
    "            valid_loss += loss.item()\n",
    "            valid_n += y_hat.shape[0]\n",
    "            \n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} validation loss: {running_loss / running_n :.3f} | \",\n",
    "            f\"Accuracy: {valid_correct / valid_n :.2f} | \",\n",
    "            f\"{valid_n / (perf_counter() - tick) :6.0f} obs/sec | \",\n",
    "            f\"{perf_counter() - big_tick :.2f} s\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3aaadde310f7da2af25cef5f777dce8f13f327f179bdb50a63ba144a6985ebf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
